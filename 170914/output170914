Number of uncategorized ELVES: 0	
7000	 ELVES in train dataset. Location of a train ELVES (Section): 	64	
1000	 ELVES in test dataset.	
iteration   10, loss = 2.266612	
iteration   20, loss = 1.845866	
iteration   30, loss = 1.447496	
iteration   40, loss = 1.372578	
iteration   50, loss = 1.164216	
iteration   60, loss = 1.029809	
iteration   70, loss = 0.951150	
iteration   80, loss = 0.914719	
iteration   90, loss = 0.819241	
iteration  100, loss = 0.817431	
iteration  110, loss = 0.717213	
iteration  120, loss = 0.683476	
iteration  130, loss = 0.644515	
iteration  140, loss = 0.657935	
iteration  150, loss = 0.589249	
iteration  160, loss = 0.568273	
iteration  170, loss = 0.552152	
iteration  180, loss = 0.512373	
iteration  190, loss = 0.519495	
iteration  200, loss = 0.497324	
iteration  210, loss = 0.474018	
iteration  220, loss = 0.467518	
iteration  230, loss = 0.456788	
iteration  240, loss = 0.427968	
iteration  250, loss = 0.416884	
iteration  260, loss = 0.412492	
iteration  270, loss = 0.402953	
iteration  280, loss = 0.407912	
iteration  290, loss = 0.387111	
iteration  300, loss = 0.376219	
iteration  310, loss = 0.364450	
iteration  320, loss = 0.359352	
iteration  330, loss = 0.362653	
iteration  340, loss = 0.353644	
iteration  350, loss = 0.339362	
iteration  360, loss = 0.341583	
iteration  370, loss = 0.328215	
iteration  380, loss = 0.320938	
iteration  390, loss = 0.329165	
iteration  400, loss = 0.311604	
iteration  410, loss = 0.310556	
iteration  420, loss = 0.316116	
iteration  430, loss = 0.306671	
iteration  440, loss = 0.293389	
iteration  450, loss = 0.287605	
iteration  460, loss = 0.289205	
iteration  470, loss = 0.280950	
iteration  480, loss = 0.299476	
iteration  490, loss = 0.275294	
iteration  500, loss = 0.272522	
 983912
[torch.LongStorage of size 1]

The NN: 
	nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> output]
  (1): nn.SpatialConvolution(1 -> 16, 4x4)
  (2): nn.Tanh
  (3): nn.SpatialMaxPooling(2x2, 2,2)
  (4): nn.SpatialConvolution(16 -> 128, 4x4)
  (5): nn.Tanh
  (6): nn.SpatialMaxPooling(2x2, 2,2)
  (7): nn.Reshape(4608)
  (8): nn.Linear(4608 -> 200)
  (9): nn.Tanh
  (10): nn.Linear(200 -> 144)
  (11): nn.LogSoftMax
}

# correct for train set:	
0.92357142857143	

# correct for test set:	
0.869	

image 13 truth 	127	

image 13 guess 	127	
1	-13.297206878662	
2	-14.538770675659	
3	-13.374489784241	
4	-14.984723091125	
5	-15.506662368774	
6	-15.158900260925	
7	-14.499987602234	
8	-13.171012878418	
9	-15.206087112427	
10	-13.905155181885	
11	-13.976157188416	
12	-13.998991966248	
13	-14.105469703674	
14	-13.410255432129	
15	-13.070579528809	
16	-13.921106338501	
17	-15.471937179565	
18	-17.291757583618	
19	-16.839138031006	
20	-15.961946487427	
21	-16.645107269287	
22	-12.970257759094	
23	-14.029981613159	
24	-14.104999542236	
25	-13.980216026306	
26	-15.498084068298	
27	-15.161155700684	
28	-14.366220474243	
29	-15.937662124634	
30	-18.297008514404	
31	-18.522068023682	
32	-17.863876342773	
33	-17.55383682251	
34	-14.538537979126	
35	-14.078039169312	
36	-14.018963813782	
37	-13.252096176147	
38	-16.245487213135	
39	-15.190084457397	
40	-16.953517913818	
41	-19.248764038086	
42	-18.887451171875	
43	-18.040538787842	
44	-17.59779548645	
45	-16.761653900146	
46	-13.677640914917	
47	-14.021948814392	
48	-13.940264701843	
49	-14.162489891052	
50	-13.877343177795	
51	-15.436421394348	
52	-15.444836616516	
53	-18.400260925293	
54	-17.235496520996	
55	-17.215181350708	
56	-17.556266784668	
57	-16.861211776733	
58	-15.749181747437	
59	-13.986595153809	
60	-13.993734359741	
61	-14.988969802856	
62	-15.098901748657	
63	-16.257188796997	
64	-15.615582466125	
65	-16.013828277588	
66	-16.82544708252	
67	-16.231754302979	
68	-17.773115158081	
69	-16.456159591675	
70	-13.976926803589	
71	-13.94833278656	
72	-13.990448951721	
73	-13.926412582397	
74	-15.826018333435	
75	-15.865772247314	
76	-16.561010360718	
77	-14.963649749756	
78	-14.923767089844	
79	-13.822195053101	
80	-14.85437297821	
81	-14.71123790741	
82	-13.081619262695	
83	-13.98671913147	
84	-13.991178512573	
85	-13.382344245911	
86	-13.906863212585	
87	-14.151341438293	
88	-14.219653129578	
89	-11.956666946411	
90	-12.11820602417	
91	-10.640911102295	
92	-11.524710655212	
93	-13.785552978516	
94	-14.674200057983	
95	-14.032280921936	
96	-14.036787033081	
97	-12.897789001465	
98	-14.769218444824	
99	-13.806121826172	
100	-15.621626853943	
101	-12.357811927795	
102	-11.068519592285	
103	-8.3595571517944	
104	-8.4905385971069	
105	-10.872654914856	
106	-11.483501434326	
107	-14.000301361084	
108	-14.05829334259	
109	-11.906883239746	
110	-10.814511299133	
111	-11.199249267578	
112	-12.46182346344	
113	-8.9135265350342	
114	-8.3823909759521	
115	-5.586905002594	
116	-6.5133242607117	
117	-8.6042852401733	
118	-10.942308425903	
119	-14.039598464966	
120	-13.986077308655	
121	-11.811694145203	
122	-10.978302001953	
123	-10.734840393066	
124	-9.6467571258545	
125	-7.9474558830261	
126	-4.4003934860229	
127	-0.29454612731934	
128	-4.6477193832397	
129	-8.3829040527344	
130	-11.465933799744	
131	-13.993226051331	
132	-13.923215866089	
133	-11.84731388092	
134	-11.903561592102	
135	-11.897066116333	
136	-8.6401805877686	
137	-8.018105506897	
138	-3.6620445251465	
139	-1.6178302764893	
140	-6.5143232345581	
141	-9.0634641647339	
142	-12.816728591919	
143	-14.063147544861	
144	-14.036610603333	

Correct: 	869	86.9 % 	
classe #   truth #  prediction #  performance #  perf/truth (%)	
   	1	0	0	0	nan	
   	2	11	12	11	100	
   	3	11	10	10	90.909090909091	
   	4	5	5	5	100	
   	5	13	12	12	92.307692307692	
   	6	13	12	11	84.615384615385	
   	7	10	12	10	100	
   	8	8	8	8	100	
   	9	8	9	8	100	
   	10	1	1	1	100	
   	11	0	0	0	nan	
   	12	0	0	0	nan	
   	13	3	3	3	100	
   	14	7	6	6	85.714285714286	
   	15	11	13	10	90.909090909091	
   	16	11	11	11	100	
   	17	4	4	4	100	
   	18	3	7	3	100	
   	19	19	16	16	84.210526315789	
   	20	8	8	8	100	
   	21	12	13	10	83.333333333333	
   	22	8	8	7	87.5	
   	23	0	0	0	nan	
   	24	0	0	0	nan	
   	25	13	12	12	92.307692307692	
   	26	11	12	11	100	
   	27	12	11	10	83.333333333333	
   	28	9	10	9	100	
   	29	7	7	7	100	
   	30	8	6	6	75	
   	31	4	4	4	100	
   	32	7	7	6	85.714285714286	
   	33	10	8	8	80	
   	34	7	6	6	85.714285714286	
   	35	0	0	0	nan	
   	36	0	0	0	nan	
   	37	13	13	13	100	
   	38	9	9	9	100	
   	39	10	10	10	100	
   	40	14	15	13	92.857142857143	
   	41	7	8	7	100	
   	42	13	13	12	92.307692307692	
   	43	8	7	7	87.5	
   	44	10	12	10	100	
   	45	10	9	8	80	
   	46	8	9	7	87.5	
   	47	0	0	0	nan	
   	48	0	0	0	nan	
   	49	8	7	7	87.5	
   	50	8	11	8	100	
   	51	14	12	12	85.714285714286	
   	52	6	5	4	66.666666666667	
   	53	8	8	7	87.5	
   	54	3	4	3	100	
   	55	7	7	7	100	
   	56	13	14	13	100	
   	57	9	8	8	88.888888888889	
   	58	13	14	12	92.307692307692	
   	59	0	0	0	nan	
   	60	0	0	0	nan	
   	61	8	8	7	87.5	
   	62	14	11	11	78.571428571429	
   	63	8	8	6	75	
   	64	9	9	8	88.888888888889	
   	65	6	6	5	83.333333333333	
   	66	5	4	4	80	
   	67	10	9	9	90	
   	68	6	8	6	100	
   	69	12	11	10	83.333333333333	
   	70	4	4	3	75	
   	71	0	0	0	nan	
   	72	0	0	0	nan	
   	73	5	6	5	100	
   	74	8	8	8	100	
   	75	4	7	4	100	
   	76	7	6	6	85.714285714286	
   	77	7	7	6	85.714285714286	
   	78	9	10	9	100	
   	79	6	7	6	100	
   	80	7	7	7	100	
   	81	9	7	7	77.777777777778	
   	82	7	7	6	85.714285714286	
   	83	0	0	0	nan	
   	84	0	0	0	nan	
   	85	8	6	6	75	
   	86	9	8	8	88.888888888889	
   	87	4	3	3	75	
   	88	7	6	6	85.714285714286	
   	89	11	10	10	90.909090909091	
   	90	10	10	10	100	
   	91	10	10	9	90	
   	92	10	13	10	100	
   	93	11	9	9	81.818181818182	
   	94	7	8	7	100	
   	95	0	0	0	nan	
   	96	0	0	0	nan	
   	97	4	7	4	100	
   	98	4	5	4	100	
   	99	5	3	3	60	
   	100	11	14	10	90.909090909091	
   	101	10	10	10	100	
   	102	5	5	5	100	
   	103	11	11	10	90.909090909091	
   	104	7	6	5	71.428571428571	
   	105	11	12	11	100	
   	106	7	7	7	100	
   	107	0	0	0	nan	
   	108	0	0	0	nan	
   	109	8	5	4	50	
   	110	13	10	10	76.923076923077	
   	111	7	6	6	85.714285714286	
   	112	8	8	7	87.5	
   	113	6	6	6	100	
   	114	5	5	5	100	
   	115	13	12	12	92.307692307692	
   	116	5	3	3	60	
   	117	7	9	7	100	
   	118	9	9	9	100	
   	119	0	0	0	nan	
   	120	0	0	0	nan	
   	121	6	0	0	0	
   	122	2	0	0	0	
   	123	10	6	5	50	
   	124	13	9	9	69.230769230769	
   	125	8	10	8	100	
   	126	9	9	8	88.888888888889	
   	127	12	12	11	91.666666666667	
   	128	9	8	8	88.888888888889	
   	129	9	9	8	88.888888888889	
   	130	9	9	9	100	
   	131	0	0	0	nan	
   	132	0	0	0	nan	
   	133	2	0	0	0	
   	134	9	0	0	0	
   	135	7	0	0	0	
   	136	8	57	8	100	
   	137	9	3	3	33.333333333333	
   	138	14	8	8	57.142857142857	
   	139	12	13	12	100	
   	140	2	2	2	100	
   	141	5	5	5	100	
   	142	6	6	6	100	
   	143	0	0	0	nan	
   	144	0	0	0	nan	
